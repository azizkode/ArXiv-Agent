#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ArXivæ™ºèƒ½ç§‘ç ”åŠ©ç† v7.0 (Personalized Profile Edition)
æ–°å¢åŠŸèƒ½ï¼š
1. æ·±åº¦æºç æ‰«æï¼šä½¿ç”¨æ­£ç¡®çš„ArXivæºç åœ°å€ (https://arxiv.org/src/...)
2. åŒæ—¶æ£€æµ‹ä¼šè®®æ¨¡æ¿å’ŒGitHubé“¾æ¥ï¼ˆä»LaTeXæºç ä¸­æŒ–æ˜ï¼‰
3. åŒºåˆ†æ‘˜è¦ä¸­çš„GitHubé“¾æ¥å’Œæºç ä¸­å‘ç°çš„éšè—é“¾æ¥
4. å®è§‚è¶‹åŠ¿ç»Ÿè®¡ï¼šç»Ÿè®¡å½“å¤©æ•´ä¸ªé¢†åŸŸï¼ˆå¦‚CSï¼‰çš„è®ºæ–‡åˆ†å¸ƒ
5. ç±»åˆ«æ˜ å°„ï¼šå°† cs.CV ç­‰ä»£ç æ˜ å°„ä¸ºå¯è¯»åç§°
6. å›¾è¡¨å¯è§†åŒ–
7. æ™ºèƒ½ç»¼è¿°ç”Ÿæˆ
8. [User Profile]: ä» user_profile.json åŠ è½½ç”¨æˆ·ç”»åƒï¼ˆå‘è¡¨è®°å½•ã€å…´è¶£ï¼‰
9. [Search Expansion]: åŸºäºç”¨æˆ·ç”»åƒè‡ªåŠ¨ç”Ÿæˆè¡ç”Ÿæœç´¢å…³é”®è¯
10. [Contextual Analysis]: LLMåˆ†ææ—¶ä¼šæ¯”å¯¹æ–°è®ºæ–‡ä¸ç”¨æˆ·ä»£è¡¨ä½œçš„å…³è”æ€§
11. [Source Tagging]: é‚®ä»¶ä¸­æ ‡è®°è®ºæ–‡æ¥æºæ˜¯"æ‰‹åŠ¨æœç´¢"è¿˜æ˜¯"AIæ¨è"
"""

import arxiv
import smtplib
import asyncio
import json
import html
import os
import re
import io
import tarfile
import aiohttp
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from collections import Counter
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
from datetime import datetime, timedelta
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import AsyncOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================= æ¨¡æ¿æŒ‡çº¹åº“ =================
# æ­£åˆ™è¡¨è¾¾å¼ : å¯¹åº”çš„ä¼šè®®/æœŸåˆŠåç§°
# æ³¨æ„ï¼šé¡ºåºå¾ˆé‡è¦ï¼Œæ›´å…·ä½“çš„æ¨¡å¼åº”è¯¥æ”¾åœ¨å‰é¢
TEMPLATE_SIGNATURES = {
    # è®¡ç®—æœºè§†è§‰ä¼šè®®
    r'\\usepackage.*\{cvpr\}': 'CVPR',
    r'\\usepackage.*\{iccv\}': 'ICCV',
    r'\\usepackage.*\{eccv\}': 'ECCV',
    # æœºå™¨å­¦ä¹ ä¼šè®®
    r'\\usepackage.*\{neurips.*\}': 'NeurIPS',
    r'\\usepackage.*\{nips.*\}': 'NeurIPS',  # æ—§åç§°
    r'\\usepackage.*\{iclr\d+.*\}': 'ICLR',
    r'\\usepackage.*\{iclr.*\}': 'ICLR',
    # AIä¼šè®®
    r'\\usepackage.*\{aaai.*\}': 'AAAI',
    # NLPä¼šè®®
    r'\\usepackage.*\{acl.*\}': 'ACL',
    r'\\usepackage.*\{naacl.*\}': 'NAACL',
    r'\\usepackage.*\{emnlp.*\}': 'EMNLP',
    # æœŸåˆŠ
    r'\\documentclass.*\{acmart\}': 'ACM',
    r'\\documentclass.*\{IEEEtran\}': 'IEEE',
    r'\\documentclass.*\{nature\}': 'Nature',
    r'\\documentclass.*\{llncs\}': 'Springer (LNCS)',
    # å…¶ä»–æ¨¡å¼
    r'\\usepackage.*\{jmlr\}': 'JMLR',
    r'\\usepackage.*\{icml.*\}': 'ICML',
    # æ–‡æœ¬æ¨¡å¼ï¼ˆæ”¾åœ¨æœ€åï¼Œä½œä¸ºå…œåº•ï¼‰
    r'Submitted to.*CVPR': 'CVPR',
    r'Submitted to.*ICCV': 'ICCV',
    r'Submitted to.*ECCV': 'ECCV',
    r'Submitted to.*NeurIPS': 'NeurIPS',
    r'Submitted to.*ICLR': 'ICLR',
}
# ===============================================

# ================= é¢†åŸŸä»£ç æ˜ å°„è¡¨ =================
# ç”¨äºå°† arxiv category è½¬æ¢ä¸ºå¯è¯»åç§°
CATEGORY_MAP = {
    'cs.AI': 'Artificial Intelligence',
    'cs.CL': 'Computation & Language (NLP)',
    'cs.CV': 'Computer Vision',
    'cs.LG': 'Machine Learning',
    'cs.RO': 'Robotics',
    'cs.SE': 'Software Engineering',
    'cs.CR': 'Cryptography & Security',
    'cs.DS': 'Data Structures',
    'cs.NE': 'Neural & Evol. Computing',
    'cs.MA': 'Multiagent Systems',
    'cs.SI': 'Social & Info Networks',
    'q-bio.BM': 'Biomolecules',
    'q-bio.GN': 'Genomics',
    'stat.ML': 'Machine Learning (Stat)'
}
# ===============================================

# ================= æ–°å¢ç±»ï¼šç”¨æˆ·ç”»åƒç®¡ç† =================

class UserProfileManager:
    """[æ–°å¢] ç”¨æˆ·ç”»åƒç®¡ç†å™¨"""
    
    def __init__(self, profile_path: str = "user_profile.json"):
        self.profile_path = profile_path
        self.data = self._load_profile()
        
    def _load_profile(self) -> Dict:
        """åŠ è½½JSONç”»åƒ"""
        if not os.path.exists(self.profile_path):
            print(f"[!] âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒæ–‡ä»¶: {self.profile_path}ï¼Œå°†ä»…ä½¿ç”¨åŸºç¡€æœç´¢åŠŸèƒ½ã€‚")
            return {"research_interests": [], "publications": []}
        try:
            with open(self.profile_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"[*] âœ… æˆåŠŸåŠ è½½ç”¨æˆ·ç”»åƒ")
                return data
        except Exception as e:
            print(f"[!] âŒ è¯»å–ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            return {"research_interests": [], "publications": []}

    def get_interests_str(self) -> str:
        """æ ¼å¼åŒ–å…´è¶£æè¿°"""
        return "\n".join([f"- {i}" for i in self.data.get("research_interests", [])])

    def get_publications_context(self) -> str:
        """è·å–è®ºæ–‡ä¸Šä¸‹æ–‡ä¾›LLMä½¿ç”¨"""
        pubs = self.data.get("publications", [])
        if not pubs: 
            return "ç”¨æˆ·æš‚æ— å·²å‘è¡¨è®ºæ–‡è®°å½•ã€‚"
        return "\n".join([f"- Title: {p['title']}\n  Abstract: {p['abstract'][:200]}..." for p in pubs])

    async def generate_derived_keywords(self, client: AsyncOpenAI, model: str) -> List[str]:
        """åŸºäºç”»åƒç”Ÿæˆ 3 ä¸ªè¡ç”Ÿæœç´¢è¯"""
        if not self.data.get("publications") and not self.data.get("research_interests"):
            return []
            
        print("[*] ğŸ§  [ç”»åƒ] æ­£åœ¨æ ¹æ®æ‚¨çš„å‘è¡¨è®°å½•è”æƒ³æœç´¢è¯...")
        
        prompt = f"""
ç”¨æˆ·ç”»åƒï¼š
å…´è¶£: {self.get_interests_str()}
ä»£è¡¨ä½œ:
{self.get_publications_context()}

è¯·ç”Ÿæˆ 3 ä¸ª ArXiv è‹±æ–‡æœç´¢å…³é”®è¯ (Search Queries)ã€‚
ç›®æ ‡ï¼šæ‰¾åˆ°å¯èƒ½å¼•ç”¨ç”¨æˆ·å·¥ä½œï¼Œæˆ–åœ¨æ–¹æ³•è®ºä¸Šé«˜åº¦ç›¸å…³çš„æœ€æ–°è®ºæ–‡ã€‚
ä¸è¦åªæ˜¯é‡å¤å…´è¶£è¯ï¼Œè¦å°è¯•ç»„åˆï¼ˆå¦‚ "GNN AND Protein"ï¼‰ã€‚

è¾“å‡ºä»…è¿”å›JSONåˆ—è¡¨: ["query1", "query2", "query3"]
"""
        
        try:
            resp = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            res_json = json.loads(resp.choices[0].message.content)
            keywords = []
            # å…¼å®¹ä¸åŒçš„JSON key
            for k, v in res_json.items():
                if isinstance(v, list): 
                    keywords = v
                    break
            
            print(f"    -> ğŸ§  AIè”æƒ³è¯: {keywords}")
            return keywords
        except Exception as e:
            print(f"[!] è¡ç”Ÿè¯ç”Ÿæˆå¤±è´¥: {e}")
            return []

# ===============================================

class ArXivPaperFetcher:
    """ArXivè®ºæ–‡çˆ¬å–ç±»"""
    
    def __init__(self, max_results: int = 10):
        self.max_results = max_results
        self.client = arxiv.Client()
    
    def fetch_personal_papers(self, query: str, days: int = 7) -> List[Dict]:
        """ç²¾è¯»è½¨é“ï¼šè·å–ç”¨æˆ·æ„Ÿå…´è¶£çš„ç‰¹å®šè®ºæ–‡"""
        queries = [q.strip() for q in query.replace(';', ',').split(',') if q.strip()]
        all_papers = []
        seen_ids = set()
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days) if days > 0 else None
        
        for q in queries:
            print(f"[*] ğŸ” [ç²¾è¯»] æ­£åœ¨æœç´¢ä¸ªæ€§åŒ–å†…å®¹: {q} ...")
            try:
                full_query = q
                if start_date:
                    date_query = f"submittedDate:[{start_date.strftime('%Y%m%d')}000000 TO {end_date.strftime('%Y%m%d')}235959]"
                    full_query = f"({q}) AND {date_query}"

                search = arxiv.Search(
                    query=full_query,
                    max_results=self.max_results,
                    sort_by=arxiv.SortCriterion.SubmittedDate,
                    sort_order=arxiv.SortOrder.Descending
                )
                
                for result in self.client.results(search):
                    arxiv_id = result.entry_id.split('/')[-1]
                    if arxiv_id not in seen_ids:
                        seen_ids.add(arxiv_id)
                        all_papers.append({
                            'title': result.title,
                            'authors': [a.name for a in result.authors],
                            'published': result.published.strftime('%Y-%m-%d'),
                            'summary': result.summary,
                            'arxiv_id': arxiv_id,
                            'pdf_url': result.pdf_url,
                            'query': q,
                            # é»˜è®¤åˆ†ç±»ï¼Œç¨åç”±LLMç»†åŒ–
                            'topic': result.primary_category, 
                            'github_info': None,
                            # é¢„ç•™åˆ†æå­—æ®µ
                            'title_cn': '',
                            'summary_cn': '',
                            'tldr': '',
                            'score': 0,
                            'reasoning': ''
                        })
            except Exception as e:
                print(f"[!] æœç´¢å¤±è´¥: {e}")
        return all_papers

    def fetch_papers_mixed(self, manual_queries: List[str], derived_queries: List[str], days: int = 7) -> List[Dict]:
        """æ··åˆæœç´¢ï¼šåˆå¹¶æ‰‹åŠ¨æŸ¥è¯¢å’Œè¡ç”ŸæŸ¥è¯¢ï¼Œå¹¶å»é‡"""
        all_papers = {}  # {arxiv_id: paper_dict}
        
        # 1. æ•´ç†æ‰€æœ‰æŸ¥è¯¢ï¼Œå¹¶æ ‡è®°æ¥æº
        search_tasks = []  # [(query, source_type)]
        for q in manual_queries: 
            search_tasks.append((q, "Manual"))
        for q in derived_queries: 
            search_tasks.append((q, "AI Derived"))
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days) if days > 0 else None
        
        for query_text, source_type in search_tasks:
            if not query_text.strip(): 
                continue
            print(f"[*] ğŸ” [{source_type}] æœç´¢: {query_text} ...")
            
            try:
                # æ„é€ æ—¶é—´æŸ¥è¯¢
                full_query = query_text
                if start_date:
                    date_q = f"submittedDate:[{start_date.strftime('%Y%m%d')}000000 TO {end_date.strftime('%Y%m%d')}235959]"
                    full_query = f"({query_text}) AND {date_q}"
                
                search = arxiv.Search(
                    query=full_query,
                    max_results=self.max_results,
                    sort_by=arxiv.SortCriterion.SubmittedDate,
                    sort_order=arxiv.SortOrder.Descending
                )
                
                count = 0
                for r in self.client.results(search):
                    pid = r.entry_id.split('/')[-1]
                    count += 1
                    # å»é‡é€»è¾‘ï¼šå¦‚æœå·²å­˜åœ¨ï¼Œä¸”å½“å‰æ˜¯Manualæ¥æºï¼Œè¦†ç›–æ—§çš„ï¼ˆManualä¼˜å…ˆçº§é«˜ï¼‰
                    if pid not in all_papers:
                        all_papers[pid] = {
                            'title': r.title, 
                            'authors': [a.name for a in r.authors],
                            'published': r.published.strftime('%Y-%m-%d'), 
                            'summary': r.summary,
                            'arxiv_id': pid, 
                            'pdf_url': r.pdf_url,
                            'topic': r.primary_category,
                            'source_query': query_text,
                            'source_type': source_type,  # å…³é”®ï¼šæ ‡è®°æ¥æº
                            'venue_guess': None, 
                            'github_info': None
                        }
                    elif source_type == "Manual":
                        all_papers[pid]['source_type'] = "Manual"  # å‡çº§ä¸ºæ‰‹åŠ¨
                        
            except Exception as e:
                print(f"[!] æœç´¢ '{query_text}' å¤±è´¥: {e}")
                
        return list(all_papers.values())

    def fetch_global_stats(self, category_prefix: str = "cs", days: int = 1) -> Dict[str, int]:
        """å®è§‚è½¨é“ï¼šç»Ÿè®¡æ•´ä¸ªé¢†åŸŸï¼ˆå¦‚cs.*ï¼‰ä»Šå¤©çš„è®ºæ–‡åˆ†å¸ƒ"""
        print(f"[*] ğŸ“ˆ [å®è§‚] æ­£åœ¨æ‰«æå…¨ç«™ {category_prefix} é¢†åŸŸè®ºæ–‡...")
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # æœç´¢è¯¥å¤§ç±»ä¸‹æ‰€æœ‰è®ºæ–‡ï¼Œé™åˆ¶æ•°é‡é˜²æ­¢è¶…æ—¶ (ä¾‹å¦‚å–æœ€è¿‘300ç¯‡ä½œä¸ºæ ·æœ¬)
        query = f"cat:{category_prefix}.* AND submittedDate:[{start_date.strftime('%Y%m%d')}000000 TO {end_date.strftime('%Y%m%d')}235959]"
        
        search = arxiv.Search(
            query=query,
            max_results=300, # é‡‡æ ·300ç¯‡è¶³ä»¥ä»£è¡¨ä»Šæ—¥è¶‹åŠ¿
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        categories = []
        try:
            for result in self.client.results(search):
                # åªç»Ÿè®¡ä¸»åˆ†ç±»
                categories.append(result.primary_category)
        except Exception as e:
            print(f"[!] å®è§‚ç»Ÿè®¡å¤±è´¥: {e}")
            
        print(f"[*] å®è§‚æ‰«æå®Œæˆï¼Œæ ·æœ¬æ•°: {len(categories)}")
        return Counter(categories)


class SourceInspector:
    """æºç æ·±åº¦ä¾¦æ¢ç±»ï¼šè´Ÿè´£ä¸‹è½½ LaTeX å¹¶åˆ†æä¼šè®®/æœŸåˆŠæ¨¡æ¿å’ŒGitHubé“¾æ¥"""
    
    # æ¨¡æ¿ç‰¹å¾æŒ‡çº¹ (æ­£åˆ™: æ ‡è¯†)
    TEMPLATE_SIGNATURES = {
        r'\\usepackage.*\{iclr\d*\}': 'ICLR',
        r'\\usepackage.*\{cvpr\}': 'CVPR',
        r'\\usepackage.*\{iccv\}': 'ICCV',
        r'\\usepackage.*\{neurips\d*\}': 'NeurIPS',
        r'\\usepackage.*\{nips\d*\}': 'NeurIPS',  # æ—§åç§°
        r'\\usepackage.*\{aaai\d*\}': 'AAAI',
        r'\\usepackage.*\{acl\d*\}': 'ACL',
        r'\\usepackage.*\{naacl\d*\}': 'NAACL',
        r'\\usepackage.*\{emnlp\d*\}': 'EMNLP',
        r'\\documentclass.*\{acmart\}': 'ACM',
        r'\\documentclass.*\{IEEEtran\}': 'IEEE',
        r'\\documentclass.*\{nature\}': 'Nature',
        r'\\documentclass.*\{llncs\}': 'Springer (LNCS)',
        r'\\usepackage.*\{spconf\}': 'ICASSP',
        r'\\usepackage.*\{jmlr\}': 'JMLR',
        r'\\usepackage.*\{icml.*\}': 'ICML',
    }
    
    # æ–‡ä»¶åæ¨¡å¼æ£€æµ‹ï¼ˆç”¨äºæ£€æµ‹.styç­‰æ ·å¼æ–‡ä»¶ï¼‰
    FILENAME_PATTERNS = {
        r'nips[_\-]?style\.sty': 'NeurIPS',
        r'neurips[_\-]?style\.sty': 'NeurIPS',
        r'iclr\d*\.sty': 'ICLR',
        r'cvpr\.sty': 'CVPR',
        r'iccv\.sty': 'ICCV',
        r'aaai\d*\.sty': 'AAAI',
        r'acl\.sty': 'ACL',
        r'naacl\.sty': 'NAACL',
        r'emnlp\.sty': 'EMNLP',
        r'icml\d*\.sty': 'ICML',
    }
    
    @staticmethod
    async def inspect_source(arxiv_id: str, semaphore: asyncio.Semaphore) -> tuple:
        """
        ä¸‹è½½å¹¶åˆ†ææºç ï¼ˆæ·±åº¦æ‰«æï¼‰
        æ–°å¢å‚æ•°: semaphore (ç”¨äºæ§åˆ¶å¹¶å‘)
        Return: (venue_name, found_github_url)
        """
        # ä½¿ç”¨ä¿¡å·é‡ï¼Œé™åˆ¶åŒæ—¶ä¸‹è½½çš„æ•°é‡
        async with semaphore:
            clean_id = arxiv_id.split('v')[0] if 'v' in arxiv_id else arxiv_id
            url = f"https://arxiv.org/src/{clean_id}"
            
            print(f"    ğŸ•µï¸ [Deep Scan] æ­£åœ¨è¯·æ±‚: {arxiv_id} (æ’é˜Ÿä¸­...)")
            
            detected_venue = None
            detected_github = None
            
            # ä¼ªè£… Headerï¼Œé˜²æ­¢è¢« ArXiv æ‹¦æˆª
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            
            # å¢åŠ é‡è¯•æœºåˆ¶
            for attempt in range(2):  # å°è¯•2æ¬¡
                try:
                    # å°†è¶…æ—¶å»¶é•¿åˆ° 60 ç§’
                    timeout = aiohttp.ClientTimeout(total=60, connect=10)
                    async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:
                        async with session.get(url) as resp:
                            if resp.status != 200:
                                print(f"    [!] {arxiv_id} ä¸‹è½½å¤±è´¥: HTTP {resp.status}")
                                return None, None
                            
                            content_type = resp.headers.get('Content-Type', '').lower()
                            if 'pdf' in content_type:
                                print(f"    [!] {arxiv_id} è¿”å›çš„æ˜¯PDFæ–‡ä»¶ï¼Œè·³è¿‡")
                                return None, None
                            
                            print(f"    [*] {arxiv_id} ä¸‹è½½æˆåŠŸï¼ŒContent-Type: {content_type}")
                            
                            # è¯»å–äºŒè¿›åˆ¶æµ
                            content = await resp.read()
                            
                            if len(content) == 0:
                                print(f"    [!] {arxiv_id} å†…å®¹ä¸ºç©º")
                                return None, None
                            
                            print(f"    [*] {arxiv_id} æ–‡ä»¶å¤§å°: {len(content)} bytes")
                            
                            file_obj = io.BytesIO(content)
                            
                            # å°è¯•ä½œä¸º tar.gz æ‰“å¼€
                            try:
                                with tarfile.open(fileobj=file_obj, mode="r:gz") as tar:
                                    print(f"    [*] {arxiv_id} æˆåŠŸè§£å‹tar.gzï¼Œå¼€å§‹æ‰«æ...")
                                    
                                    # ç¬¬ä¸€æ­¥ï¼šå…ˆæ£€æŸ¥æ–‡ä»¶åæ¨¡å¼ï¼ˆå¦‚ nips_style.styï¼‰
                                    if not detected_venue:
                                        for member in tar.getmembers():
                                            if member.isfile():
                                                filename = member.name.lower()
                                                for pattern, venue in SourceInspector.FILENAME_PATTERNS.items():
                                                    if re.search(pattern, filename, re.IGNORECASE):
                                                        detected_venue = venue
                                                        print(f"    âœ… [Venue] é€šè¿‡æ–‡ä»¶åæ£€æµ‹åˆ°æ¨¡æ¿: {venue} (æ–‡ä»¶: {member.name})")
                                                        break
                                                if detected_venue:
                                                    break
                                    
                                    # ç¬¬äºŒæ­¥ï¼šæ‰«æ .tex æ–‡ä»¶å†…å®¹
                                    tex_files_found = 0
                                    for member in tar.getmembers():
                                        if member.name.endswith('.tex') and member.isfile():
                                            tex_files_found += 1
                                            try:
                                                f = tar.extractfile(member)
                                                if f:
                                                    # è¯»å–æ–‡æœ¬ (å¿½ç•¥ç¼–ç é”™è¯¯)
                                                    tex_text = f.read().decode('utf-8', errors='ignore')
                                                    
                                                    # 1. åŒ¹é…ä¼šè®®æ¨¡æ¿ï¼ˆå¦‚æœæ–‡ä»¶åæ²¡æ£€æµ‹åˆ°ï¼‰
                                                    if not detected_venue:
                                                        for pattern, venue in SourceInspector.TEMPLATE_SIGNATURES.items():
                                                            if re.search(pattern, tex_text, re.IGNORECASE):
                                                                detected_venue = venue
                                                                print(f"    âœ… [Venue] é€šè¿‡å†…å®¹æ£€æµ‹åˆ°æ¨¡æ¿: {venue} (æ–‡ä»¶: {member.name})")
                                                                break
                                                    
                                                    # 2. åŒ¹é… GitHub é“¾æ¥
                                                    if not detected_github:
                                                        gh_match = re.search(r'https?://github\.com/[\w-]+/[\w.-]+', tex_text)
                                                        if gh_match:
                                                            detected_github = gh_match.group(0)
                                                            print(f"    âœ… [GitHub] åœ¨æºç ä¸­å‘ç°: {detected_github}")
                                                            
                                                    # å¦‚æœä¸¤ä¸ªéƒ½æ‰¾åˆ°äº†ï¼Œæå‰ç»“æŸå¾ªç¯
                                                    if detected_venue and detected_github:
                                                        break
                                            except Exception as e:
                                                continue
                                    
                                    if tex_files_found == 0:
                                        print(f"    [!] {arxiv_id} æœªæ‰¾åˆ°.texæ–‡ä»¶")
                                    else:
                                        print(f"    [*] {arxiv_id} æ‰«æäº† {tex_files_found} ä¸ª.texæ–‡ä»¶")
                                            
                            except (tarfile.ReadError, tarfile.CompressionError, tarfile.TarError) as e:
                                print(f"    [!] {arxiv_id} tar.gzè§£å‹å¤±è´¥: {e}ï¼Œå°è¯•ä½œä¸ºçº¯æ–‡æœ¬å¤„ç†...")
                                # å¯èƒ½æ˜¯å•ä¸ª .gz æ–‡ä»¶ï¼ˆä¸æ˜¯ tarï¼‰ï¼Œæˆ–è€…å°±æ˜¯çº¯æ–‡æœ¬
                                try:
                                    text_content = content[:10000].decode('utf-8', errors='ignore')
                                    # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒLaTeXæ–‡ä»¶
                                    if '\\documentclass' in text_content or '\\usepackage' in text_content:
                                        print(f"    [*] {arxiv_id} æ£€æµ‹åˆ°LaTeXå†…å®¹ï¼ŒåŒ¹é…æ¨¡æ¿...")
                                        # åŒ¹é…æ¨¡æ¿
                                        if not detected_venue:
                                            for pattern, venue in SourceInspector.TEMPLATE_SIGNATURES.items():
                                                if re.search(pattern, text_content, re.IGNORECASE):
                                                    detected_venue = venue
                                                    print(f"    âœ… [Venue] æ£€æµ‹åˆ°æ¨¡æ¿: {venue}")
                                                    break
                                        # åŒ¹é…GitHubé“¾æ¥
                                        if not detected_github:
                                            gh_match = re.search(r'https?://github\.com/[\w-]+/[\w.-]+', text_content)
                                            if gh_match:
                                                detected_github = gh_match.group(0)
                                                print(f"    âœ… [GitHub] åœ¨æºç ä¸­å‘ç°: {detected_github}")
                                except Exception as e2:
                                    print(f"    [!] {arxiv_id} æ–‡æœ¬è§£ç å¤±è´¥: {e2}")
                            
                            # è¾“å‡ºæœ€ç»ˆç»“æœ
                            if detected_venue:
                                print(f"    âœ… [Deep Scan] å®Œæˆ: {arxiv_id} -> Venue: {detected_venue}")
                            else:
                                print(f"    âœ… [Deep Scan] å®Œæˆ: {arxiv_id} -> æœªæ£€æµ‹åˆ°æ¨¡æ¿")
                            
                            if detected_github:
                                print(f"    âœ… [Deep Scan] GitHub: {detected_github}")
                            
                            return detected_venue, detected_github
                            
                except asyncio.TimeoutError:
                    print(f"    [!] {arxiv_id} ä¸‹è½½è¶…æ—¶ (å°è¯• {attempt+1}/2)")
                    if attempt == 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                        return None, None
                except aiohttp.ClientError as e:
                    print(f"    [!] {arxiv_id} ç½‘ç»œé”™è¯¯ (å°è¯• {attempt+1}/2): {e}")
                    if attempt == 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                        return None, None
                except Exception as e:
                    print(f"    [!] {arxiv_id} æºç åˆ†æå¼‚å¸¸ (å°è¯• {attempt+1}/2): {e}")
                    if attempt == 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                        return None, None
            
            return None, None
    
    @staticmethod
    async def detect_venue(arxiv_id: str, semaphore: asyncio.Semaphore = None) -> str:
        """å…¼å®¹æ—§æ¥å£ï¼šä»…è¿”å›venue"""
        # å¦‚æœæ²¡æœ‰æä¾›ä¿¡å·é‡ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ï¼ˆé™åˆ¶ä¸º1ï¼Œé¿å…å¹¶å‘ï¼‰
        if semaphore is None:
            semaphore = asyncio.Semaphore(1)
        venue, _ = await SourceInspector.inspect_source(arxiv_id, semaphore)
        return venue if venue else None


class PaperProcessor:
    """è®ºæ–‡æ™ºèƒ½åˆ†æå™¨ï¼šè´Ÿè´£å¹¶å‘ç¿»è¯‘ã€æ‰“åˆ†å’Œæ€»ç»“"""
    
    def __init__(self, profile_manager=None, model="gpt-3.5-turbo", user_interest=""):
        api_key = os.getenv('OPENAI_API_KEY')
        base_url = os.getenv('OPENAI_BASE_URL')
        if api_key:
            kwargs = {'api_key': api_key}
            if base_url:
                kwargs['base_url'] = base_url
            self.client = AsyncOpenAI(**kwargs)
        else:
            self.client = None
        self.model = model
        self.user_interest = user_interest
        self.profile = profile_manager  # ç”¨æˆ·ç”»åƒç®¡ç†å™¨
        # åˆ›å»ºä¸€ä¸ªå…¨å±€çš„ä¿¡å·é‡ï¼Œé™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º 3
        self.download_semaphore = asyncio.Semaphore(3)

    async def _audit_github(self, text):
        """å®¡è®¡GitHubé“¾æ¥å¹¶è·å–ä»“åº“ä¿¡æ¯"""
        url_match = re.search(r'https?://github\.com/([\w-]+/[\w.-]+)', text)
        if not url_match:
            return None
        
        full_url = url_match.group(0)
        repo_path = url_match.group(1)
        api_url = f"https://api.github.com/repos/{repo_path}"
        info = {"url": full_url, "stars": "N/A", "last_update": "N/A", "desc": "Found Link"}
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(api_url) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        info["stars"] = data.get('stargazers_count', 0)
                        info["last_update"] = data.get('pushed_at', '').split('T')[0]
                        info["desc"] = "âœ… Repo Found"
                    elif resp.status == 404:
                        info["desc"] = "âš ï¸ 404 Not Found"
        except:
            pass
        return info

    async def analyze_paper_async(self, paper: Dict) -> Dict:
        """å¼‚æ­¥å¤„ç†å•ç¯‡è®ºæ–‡ï¼šä¸€æ¬¡è°ƒç”¨å®Œæˆç¿»è¯‘ã€æ‰“åˆ†ã€æ€»ç»“å’Œæ¨¡æ¿æ£€æµ‹"""
        if not self.client or not self.client.api_key:
            # æ— Keyç›´æ¥è¿”å›åŸæ•°æ®
            paper['title_cn'] = paper['title']
            paper['summary_cn'] = paper['summary']
            paper['tldr'] = 'AIåˆ†æä¸å¯ç”¨'
            paper['score'] = 5
            paper['reasoning'] = 'æœªé…ç½®APIå¯†é’¥'
            paper['topic'] = paper.get('topic', 'Unknown')
            paper['github_info'] = None
            # å³ä½¿æ²¡æœ‰APIï¼Œä¹Ÿå°è¯•æ£€æµ‹æ¨¡æ¿ï¼ˆä½¿ç”¨ä¸´æ—¶çš„ä¿¡å·é‡ï¼‰
            temp_semaphore = asyncio.Semaphore(1)
            paper['venue_guess'] = await SourceInspector.detect_venue(paper['arxiv_id'], temp_semaphore)
            return paper

        # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ï¼šLLMåˆ†æã€æºç æ·±åº¦æ‰«æï¼ˆåŒ…å«venueå’ŒGitHubæ£€æµ‹ï¼‰
        # æºç æ‰«æä¼šåŒæ—¶æ£€æµ‹venueå’ŒGitHubé“¾æ¥
        # ä¼ é€’ä¿¡å·é‡ç»™ SourceInspectorï¼Œé™åˆ¶å¹¶å‘ä¸‹è½½æ•°é‡
        source_task = SourceInspector.inspect_source(paper['arxiv_id'], self.download_semaphore)
        
        # æ„é€ æç¤ºè¯ï¼ˆåŒ…å«ç”¨æˆ·ç”»åƒï¼‰
        profile_context = ""
        if self.profile:
            profile_context = f"""
ã€å½“å‰ç”¨æˆ·ç”»åƒã€‘
ç ”ç©¶å…´è¶£: {self.profile.get_interests_str()}
ç”¨æˆ·ä»£è¡¨ä½œ:
{self.profile.get_publications_context()}

ç‰¹åˆ«è¦æ±‚ï¼š
1. å…³è”æ€§åˆ†æ (Contextual Analysis): å¿…é¡»å°†æ–°è®ºæ–‡ä¸ã€ç”¨æˆ·ä»£è¡¨ä½œã€‘è¿›è¡Œæ¯”å¯¹ã€‚å¦‚æœæ–°è®ºæ–‡å¼•ç”¨äº†ç±»ä¼¼æ–¹æ³•ã€è§£å†³äº†ç”¨æˆ·è®ºæ–‡ä¸­çš„é—ç•™é—®é¢˜ï¼Œæˆ–å±äºåŒä¸€æŠ€æœ¯è·¯çº¿ï¼ˆå¦‚Mamba/GNN/MLLMï¼‰ï¼Œè¯·åœ¨ reasoning ä¸­æ˜ç¡®æŒ‡å‡ºï¼ˆä¾‹å¦‚ï¼š"æ­¤æ–‡æ‰©å±•äº†æ‚¨å…³äºMambaçš„ç ”ç©¶..."ï¼‰ã€‚
2. æ‰“åˆ† (Score): åŸºäºä¸ç”¨æˆ·ç”»åƒçš„å¥‘åˆåº¦æ‰“åˆ† (0-10)ã€‚

"""
        
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©ç†ã€‚
        
        {profile_context}
        
        ç”¨æˆ·çš„ç ”ç©¶å…´è¶£æ˜¯ï¼š

        ---

        {self.user_interest}

        ---

        è¯·é˜…è¯»ç»™å®šçš„è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

        1. Translate: å°†æ ‡é¢˜å’Œæ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼ˆä¸“ä¸šå­¦æœ¯é£æ ¼ï¼‰ã€‚

        2. Score: æ ¹æ®ç”¨æˆ·å…´è¶£å’Œç”»åƒå¯¹è®ºæ–‡ç›¸å…³æ€§è¿›è¡Œæ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ã€‚

        3. TLDR: ç”¨ä¸­æ–‡å†™ä¸€å¥è¯çš„"å¤ªé•¿ä¸çœ‹"æ€»ç»“ï¼Œç›´æ¥æŒ‡å‡ºè®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€‚

        4. Topic: æå–è®ºæ–‡çš„æ ¸å¿ƒä¸»é¢˜ï¼ˆç®€çŸ­çŸ­è¯­ï¼Œå¦‚"å›¾ç¥ç»ç½‘ç»œ"ã€"å¤šæ¨¡æ€å­¦ä¹ "ï¼‰ã€‚

        5. Reasoning: ç®€çŸ­è¯´æ˜æ‰“åˆ†ç†ç”±ï¼Œå¦‚æœä¸ç”¨æˆ·ä»£è¡¨ä½œç›¸å…³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚

        è¯·ä¸¥æ ¼ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:

        - title_cn (string)
        - summary_cn (string)
        - score (integer)
        - tldr (string)
        - topic (string)
        - reasoning (string)
        """

        user_content = f"Title: {paper['title']}\nAbstract: {paper['summary']}"

        try:
            # åˆ›å»ºLLMä»»åŠ¡ï¼ˆè¿™æ˜¯ä¸€ä¸ªåç¨‹ï¼‰
            llm_task = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆï¼ˆLLMåˆ†æã€æºç æ·±åº¦æ‰«æï¼‰
            # ä½¿ç”¨return_exceptions=Trueç¡®ä¿å³ä½¿æŸä¸ªä»»åŠ¡å¤±è´¥ï¼Œå…¶ä»–ä»»åŠ¡ä¹Ÿèƒ½å®Œæˆ
            results = await asyncio.gather(llm_task, source_task, return_exceptions=True)
            res, source_result = results
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
            if isinstance(res, Exception):
                raise res
            if isinstance(source_result, Exception):
                source_result = (None, None)
            
            # è§£ææºç æ‰«æç»“æœ
            venue, deep_github = source_result if source_result else (None, None)
            print(f"    [DEBUG] æºç æ‰«æç»“æœ: venue={venue}, github={deep_github}")
            
            # è§£æLLMå“åº”
            if hasattr(res, 'choices') and len(res.choices) > 0:
                data = json.loads(res.choices[0].message.content)
            else:
                raise ValueError("LLMå“åº”æ ¼å¼é”™è¯¯")
            
            # åˆå¹¶GitHubé“¾æ¥ï¼šä¼˜å…ˆä½¿ç”¨æ‘˜è¦é‡Œçš„ï¼Œå¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨æºç é‡ŒæŒ–å‡ºæ¥çš„
            final_github = None
            is_hidden_github = False
            
            # å…ˆçœ‹æ‘˜è¦é‡Œæœ‰æ²¡æœ‰
            summary_gh_match = re.search(r'https?://github\.com/[\w-]+/[\w.-]+', paper['summary'])
            if summary_gh_match:
                final_github = summary_gh_match.group(0)
            elif deep_github:
                final_github = deep_github  # ä½¿ç”¨æºç é‡ŒæŒ–å‡ºæ¥çš„
                is_hidden_github = True
                print(f"    ğŸ‰ [æƒŠå–œ] åœ¨æ­£æ–‡ä¸­å‘ç°äº†éšè—çš„ GitHub: {final_github}")
            
            # è·å–GitHubè¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰é“¾æ¥ï¼‰
            github_info = None
            if final_github:
                # å¼‚æ­¥è·å–GitHubä»“åº“ä¿¡æ¯
                github_info = await self._audit_github(final_github)
                if github_info:
                    github_info['is_hidden'] = is_hidden_github
                else:
                    github_info = {
                        "url": final_github,
                        "desc": "Link Found",
                        "stars": "N/A",
                        "last_update": "N/A",
                        "is_hidden": is_hidden_github
                    }
            
            # æ›´æ–°paperå­—å…¸
            paper['title_cn'] = data.get('title_cn', paper['title'])
            paper['summary_cn'] = data.get('summary_cn', paper['summary'])
            paper['score'] = data.get('score', 0)
            paper['tldr'] = data.get('tldr', 'æš‚æ— æ€»ç»“')
            paper['topic'] = data.get('topic', paper.get('topic', 'Unknown'))
            paper['reasoning'] = data.get('reasoning', '')
            paper['github_info'] = github_info
            paper['venue_guess'] = venue  # ä¿å­˜ä¾¦æ¢ç»“æœ
            print(f"    [DEBUG] ä¿å­˜åˆ°paper: venue_guess={venue}, github_info={github_info is not None}")
            
            return paper

        except Exception as e:
            print(f"[!] åˆ†æè®ºæ–‡ {paper['arxiv_id']} å¤±è´¥: {e}")
            # å¤±è´¥å›é€€ï¼šè‡³å°‘ä¿ç•™åŸæ–‡
            paper['title_cn'] = paper['title']
            paper['summary_cn'] = "AIåˆ†æå¤±è´¥ï¼Œæ˜¾ç¤ºåŸæ–‡ã€‚\n" + paper['summary']
            paper['tldr'] = 'åˆ†æå¤±è´¥'
            paper['score'] = 0
            paper['reasoning'] = f'åˆ†æé”™è¯¯: {str(e)}'
            paper['topic'] = paper.get('topic', 'Unknown')
            
            # ç¡®ä¿ä¹‹å‰åˆ›å»ºçš„ä»»åŠ¡è¢«awaitï¼ˆå¦‚æœè¿˜æ²¡æœ‰å®Œæˆï¼‰
            # å³ä½¿LLMå¤±è´¥ï¼Œä¹Ÿå°è¯•è·å–æºç æ‰«æç»“æœ
            try:
                # å¦‚æœä¹‹å‰çš„ä»»åŠ¡è¿˜æ²¡æœ‰å®Œæˆï¼Œç­‰å¾…å®ƒ
                if 'source_task' in locals():
                    source_result = await asyncio.gather(source_task, return_exceptions=True)[0]
                    if not isinstance(source_result, Exception):
                        venue, deep_github = source_result if source_result else (None, None)
                        paper['venue_guess'] = venue
                        # å°è¯•è·å–GitHubä¿¡æ¯
                        if deep_github:
                            github_info = await self._audit_github(deep_github)
                            if github_info:
                                github_info['is_hidden'] = True
                            else:
                                github_info = {"url": deep_github, "desc": "Link Found", "is_hidden": True}
                            paper['github_info'] = github_info
                        else:
                            # å°è¯•ä»æ‘˜è¦ä¸­æå–
                            summary_gh_match = re.search(r'https?://github\.com/[\w-]+/[\w.-]+', paper['summary'])
                            if summary_gh_match:
                                github_info = await self._audit_github(summary_gh_match.group(0))
                                if github_info:
                                    github_info['is_hidden'] = False
                                paper['github_info'] = github_info
                else:
                    # å¦‚æœä»»åŠ¡è¿˜æ²¡æœ‰åˆ›å»ºï¼Œåˆ›å»ºæ–°ä»»åŠ¡
                    source_task = SourceInspector.inspect_source(paper['arxiv_id'], self.download_semaphore)
                    source_result = await asyncio.gather(source_task, return_exceptions=True)[0]
                    if not isinstance(source_result, Exception):
                        venue, deep_github = source_result if source_result else (None, None)
                        paper['venue_guess'] = venue
                        if deep_github:
                            github_info = await self._audit_github(deep_github)
                            if github_info:
                                github_info['is_hidden'] = True
                            paper['github_info'] = github_info
            except Exception as e2:
                print(f"[!] è·å–æºç ä¿¡æ¯å¤±è´¥: {e2}")
                paper['github_info'] = None
                paper['venue_guess'] = None
            return paper

    async def generate_briefing(self, papers: List[Dict], global_stats_desc: str) -> str:
        """ç»“åˆäº†ä¸ªäººè®ºæ–‡å’Œå®è§‚è¶‹åŠ¿çš„ç»¼è¿°"""
        context = "\n".join([f"- {p.get('title_cn', p['title'])} (Topic: {p.get('topic', 'Unknown')})" for p in papers[:5]])
        
        prompt = f"""
        ä½ æ˜¯ç§‘ç ”æƒ…æŠ¥ä¸“å®¶ã€‚

        ã€å®è§‚æ•°æ®ã€‘
        ä»Šå¤©ArXivå…¨ç«™è®¡ç®—æœºé¢†åŸŸçš„çƒ­é—¨æ–¹å‘åˆ†å¸ƒï¼š{global_stats_desc}

        ã€ç”¨æˆ·ä¸ªæ€§åŒ–ç²¾é€‰ã€‘
        {context}

        è¯·å†™ä¸€æ®µ"ArXivæ—©æŠ¥"ã€‚
        1. å…ˆç”¨ä¸€å¥è¯æ¦‚æ‹¬ä»Šå¤©çš„å®è§‚å¤§ç›˜ï¼ˆå“ªä¸ªå­é¢†åŸŸæœ€ç«ï¼‰ã€‚
        2. å†ä»‹ç»ç”¨æˆ·å…³æ³¨çš„é¢†åŸŸæœ‰ä»€ä¹ˆæ–°çªç ´ã€‚
        3. é£æ ¼ç®€ç»ƒä¸“ä¸šã€‚
        """
        
        if not self.client or not self.client.api_key:
            return "AIç»¼è¿°ç”Ÿæˆä¸å¯ç”¨ï¼ˆæœªé…ç½®APIå¯†é’¥ï¼‰"
        
        try:
            resp = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            return resp.choices[0].message.content
        except Exception as e:
            print(f"[!] ç»¼è¿°ç”Ÿæˆå¤±è´¥: {e}")
            return "ç»¼è¿°ç”Ÿæˆå¤±è´¥"

    async def process_batch(self, papers: List[Dict]) -> List[Dict]:
        """æ‰¹é‡å¹¶å‘å¤„ç†"""
        print(f"[*] å¼€å§‹å¹¶è¡Œåˆ†æ {len(papers)} ç¯‡è®ºæ–‡ (ä½¿ç”¨æ¨¡å‹: {self.model})...")
        tasks = [self.analyze_paper_async(paper) for paper in papers]
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        processed_papers = await asyncio.gather(*tasks)
        print("[*] åˆ†æå®Œæˆï¼")
        
        # æŒ‰ç…§ç›¸å…³æ€§åˆ†æ•°é™åºæ’åºï¼Œåˆ†æ•°é«˜çš„æ’å‰é¢
        processed_papers.sort(key=lambda x: x.get('score', 0), reverse=True)
        return processed_papers


class Visualizer:
    """å›¾è¡¨å¯è§†åŒ–ç±»"""
    
    @staticmethod
    def draw_global_trend(category_counts: Dict[str, int]) -> bytes:
        """ç»˜åˆ¶å®è§‚è¶‹åŠ¿å›¾"""
        if not category_counts:
            return None
        
        # 1. æ˜ å°„åç§° (cs.CV -> Computer Vision)
        mapped_counts = {}
        for code, count in category_counts.items():
            # å–å‰ä¸¤ä¸ªåˆ†æ®µï¼Œä¾‹å¦‚ cs.CV
            name = CATEGORY_MAP.get(code, code) 
            mapped_counts[name] = mapped_counts.get(name, 0) + count
            
        # 2. æ’åºå¹¶å– Top 10
        sorted_stats = sorted(mapped_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        if not sorted_stats:
            return None
        
        labels, sizes = zip(*sorted_stats)
        
        # 3. ç»˜å›¾ (æ°´å¹³æŸ±çŠ¶å›¾ï¼Œé€‚åˆé•¿æ ‡ç­¾)
        fig, ax = plt.subplots(figsize=(10, 6))
        y_pos = range(len(labels))
        
        # é…è‰²ï¼šæ„å»ºæ¸å˜è‰²
        import numpy as np
        color_range = plt.cm.GnBu(np.linspace(0.4, 0.9, len(labels)))
        
        bars = ax.barh(y_pos, sizes, align='center', color=color_range)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(labels)
        ax.invert_yaxis()  # æœ€å¤§çš„åœ¨æœ€ä¸Šé¢
        
        # æ•°æ®æ ‡ç­¾
        for i, v in enumerate(sizes):
            ax.text(v + 0.5, i, str(v), color='#333', va='center', fontweight='bold')
            
        ax.set_title(f"ArXiv Global Trend: Top Areas Today (Sampled)", fontsize=14, pad=20)
        ax.spines['right'].set_visible(False)
        ax.spines['top'].set_visible(False)
        plt.tight_layout()
        
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        plt.close(fig)
        buf.seek(0)
        return buf.getvalue()


class EmailSender:
    """é‚®ä»¶å‘é€ç±» (ä¼˜åŒ–HTMLæ¨¡æ¿)"""
    
    def __init__(self, smtp_server, smtp_port, sender_email, sender_password):
        self.smtp_server = smtp_server
        self.smtp_port = smtp_port
        self.sender_email = sender_email
        self.sender_password = sender_password
        
    def _generate_html(self, papers: List[Dict], query: str, briefing: str, derived_queries: List[str] = None, has_chart: bool = False) -> str:
        """ç”Ÿæˆç¾åŒ–çš„HTMLå†…å®¹"""
        if derived_queries is None:
            derived_queries = []
        
        # HTML å¤´éƒ¨æ ·å¼
        chart_section = ""
        if has_chart:
            chart_section = """
                <div style="text-align:center; margin:20px 0;">
                    <h3>ğŸ“Š Global Category Trends (Today)</h3>
                    <img src="cid:trend_chart" style="max-width:100%; border:1px solid #eee; border-radius:8px;">
                    <p style="font-size:12px; color:#999;">Statistics based on broad field sampling</p>
                </div>
            """
        
        html_content = f"""
        <html>
        <head>
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; }}
                .container {{ max-width: 800px; margin: 0 auto; padding: 20px; }}
                .header {{ background-color: #f8f9fa; padding: 20px; border-bottom: 3px solid #0056b3; margin-bottom: 20px; }}
                .paper-card {{ border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }}
                .high-score {{ border-left: 5px solid #28a745; background-color: #fcffff; }}
                .med-score {{ border-left: 5px solid #ffc107; }}
                .low-score {{ border-left: 5px solid #dc3545; opacity: 0.9; }}
                .title {{ color: #0056b3; text-decoration: none; font-size: 18px; font-weight: bold; }}
                .meta {{ font-size: 12px; color: #666; margin-bottom: 10px; }}
                .score-badge {{ display: inline-block; padding: 2px 8px; border-radius: 4px; color: white; font-weight: bold; font-size: 12px; margin-right: 10px; }}
                .tldr {{ background-color: #eef2f7; padding: 10px; border-radius: 4px; font-style: italic; margin: 10px 0; border-left: 3px solid #0056b3; }}
                .abstract {{ font-size: 14px; text-align: justify; }}
                .footer {{ text-align: center; font-size: 12px; color: #999; margin-top: 30px; }}
                .briefing {{ background-color: #f9f9f9; padding: 15px; border-radius: 8px; margin-bottom: 20px; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h2>ğŸ•µï¸ ArXiv Agent v7.0 Report</h2>
                    <p>Search Query: <strong>{html.escape(query)}</strong> | Found: {len(papers)} papers</p>
                    <p style="font-size:12px; color:#666;">Papers sorted by AI Relevance Score | Venue Detection Enabled | Personalized Profile Edition</p>
                </div>
                
                <div class="briefing">
                    <h3>ğŸ¤– Agent's Briefing</h3>
                    <p>{html.escape(briefing).replace(chr(10), '<br>')}</p>
                    {f'<div style="margin-top:10px; font-size:12px; color:#586069; border-top:1px solid #e1e4e8; padding-top:10px;"><b>Auto-Expanded Search:</b> {", ".join([f"<code>{html.escape(q)}</code>" for q in derived_queries]) if derived_queries else "None"}</div>' if derived_queries else ''}
                </div>
                
                {chart_section}
        """
        
        for p in papers:
            # 1. é¢œè‰²ç¼–ç åˆ†æ•°
            score = p.get('score', 0)
            score_class = "high-score" if score >= 8 else ("med-score" if score >= 5 else "low-score")
            score_color = "#28a745" if score >= 8 else ("#ffc107" if score >= 5 else "#dc3545")
            
            # 2. HTML è½¬ä¹‰
            title_display = html.escape(p.get('title_cn', p['title']))
            summary_display = html.escape(p.get('summary_cn', p.get('summary', ''))).replace('\n', '<br>')
            tldr_display = html.escape(p.get('tldr', ''))
            authors_display = html.escape(", ".join(p['authors'][:5])) + ("..." if len(p['authors'])>5 else "")
            reasoning = html.escape(p.get('reasoning', ''))
            
            # [UIå¢å¼º] æ¥æº Badge
            src_badge = ""
            src_type = p.get('source_type', 'Manual')
            if src_type == "Manual":
                src_text = "ğŸ¯ Manual"
                src_style = "background:#e1ecf4; color:#39739d;"
            else:
                src_text = "ğŸ§  AI Derived"
                src_style = "background:#f0f4c3; color:#827717;"
            
            src_badge = f"<span style='{src_style} padding:2px 6px; border-radius:4px; font-size:11px; margin-right:5px; border:1px solid rgba(0,0,0,0.1);'>{src_text}</span>"
            
            # GitHubä¿¡æ¯
            github_badge = ""
            if p.get('github_info'):
                info = p['github_info']
                # åŒºåˆ†æ‘˜è¦ä¸­çš„é“¾æ¥å’Œæºç ä¸­å‘ç°çš„é“¾æ¥
                if info.get('is_hidden'):
                    badge_text = "ğŸ•µï¸ Code (Found in Source)"
                    badge_color = "#2ea44f"  # GitHubç»¿
                else:
                    badge_text = "ğŸ“¦ Code (In Abstract)"
                    badge_color = "#0366d6"  # è“è‰²
                
                github_badge = f"""
                <span style="background:{badge_color}; color:white; padding:2px 6px; border-radius:4px; font-size:11px; margin-right:5px;" title="{info.get('url', '')}">
                    {badge_text} | â­ {info.get('stars', 'N/A')}
                </span>
                """
            
            # Venue Badge (ä¼šè®®/æœŸåˆŠæ¨¡æ¿æ£€æµ‹)
            venue_badge = ""
            venue_name = p.get('venue_guess')
            # è°ƒè¯•ï¼šæ‰“å°venue_nameçš„å€¼
            # print(f"[DEBUG] Paper {p['arxiv_id']}: venue_guess = {venue_name}")
            
            # ä¿®å¤ï¼švenue_nameå¯èƒ½æ˜¯Noneï¼Œéœ€è¦æ£€æŸ¥
            if venue_name and venue_name not in [None, "Unknown", "Error", "No Source", "PDF Only", "Unknown Template", ""]:
                # ä¸åŒçš„ä¼šè®®ç”¨ä¸åŒçš„é¢œè‰²
                bg_color = "#6f42c1"  # ç´«è‰²é»˜è®¤
                if "CVPR" in venue_name:
                    bg_color = "#0366d6"  # è“
                elif "NeurIPS" in venue_name:
                    bg_color = "#b60205"  # çº¢
                elif "ICLR" in venue_name:
                    bg_color = "#d9534f"  # æµ…çº¢
                elif "ICCV" in venue_name:
                    bg_color = "#28a745"  # ç»¿
                elif "ECCV" in venue_name:
                    bg_color = "#17a2b8"  # é’
                elif "AAAI" in venue_name:
                    bg_color = "#ffc107"  # é»„
                elif "ACL" in venue_name or "NAACL" in venue_name or "EMNLP" in venue_name:
                    bg_color = "#dc3545"  # çº¢
                elif "ACM" in venue_name:
                    bg_color = "#007bff"  # è“
                elif "IEEE" in venue_name:
                    bg_color = "#00629b"  # æ·±è“
                elif "ICML" in venue_name:
                    bg_color = "#e83e8c"  # ç²‰çº¢
                elif "JMLR" in venue_name:
                    bg_color = "#20c997"  # é’ç»¿
                
                venue_badge = f"""
                <span style="background:{bg_color}; color:white; padding:2px 6px; border-radius:4px; font-size:11px; font-weight:bold; margin-right:5px;" title="Detected via LaTeX Source">
                    ğŸ›ï¸ {venue_name}
                </span>
                """
            # æ˜¾ç¤ºæ‰«æçŠ¶æ€ï¼ˆå³ä½¿æ²¡æœ‰æ£€æµ‹åˆ°æ¨¡æ¿ï¼‰
            elif venue_name is None:
                # æ˜¾ç¤º"å·²æ‰«æä½†æœªæ£€æµ‹åˆ°"
                venue_badge = f"""
                <span style="background:#f0f0f0; color:#666; padding:2px 6px; border-radius:4px; font-size:11px;" title="Source scanned but no template detected">
                    ğŸ” Scanned
                </span>
                """
            
            html_content += f"""
                <div class="paper-card {score_class}">
                    <div style="margin-bottom: 8px;">
                        <span class="score-badge" style="background-color: {score_color};">Score: {score}/10</span>
                        {src_badge}
                        {venue_badge}
                        {github_badge}
                        <span style="color:#666; font-size:12px; float:right;">{p['published']}</span>
                    </div>
                    <a href="{p['pdf_url']}" class="title">{title_display}</a>
                    <div class="meta">
                        <strong>ID:</strong> {p['arxiv_id']} | <strong>Topic:</strong> {html.escape(p.get('topic', 'Unknown'))}<br>
                        <strong>Authors:</strong> {authors_display}
                    </div>
                    <div class="tldr">
                        <strong>ğŸ’¡ TL;DR:</strong> {tldr_display}
                    </div>
                    <div class="abstract">
                        <details>
                            <summary style="cursor: pointer; color: #0056b3;">Read Abstract (ç‚¹å‡»å±•å¼€æ‘˜è¦)</summary>
                            <p>{summary_display}</p>
                            <hr>
                            <p style="font-size:12px; color:#999;">Original Title: {html.escape(p['title'])}</p>
                        </details>
                    </div>
                </div>
            """
            
        html_content += """
                <div class="footer">
                    Generated by ArXiv Agent v7.0 (Powered by GPT) | Personalized Profile Edition
                </div>
            </div>
        </body>
        </html>
        """
        return html_content

    def send_email(self, recipient_email: str, subject: str, papers: List[Dict], query: str, briefing: str, derived_queries: List[str] = None, chart_img: bytes = None):
        """å‘é€é‚®ä»¶ï¼Œæ”¯æŒåµŒå…¥å›¾è¡¨"""
        if derived_queries is None:
            derived_queries = []
        try:
            # åˆ›å»ºrelatedç±»å‹çš„multipartï¼Œç”¨äºåµŒå…¥å›¾ç‰‡
            msg = MIMEMultipart('related')
            msg['From'] = self.sender_email
            msg['To'] = recipient_email
            msg['Subject'] = subject
            
            # åˆ›å»ºalternativeéƒ¨åˆ†ç”¨äºHTMLå†…å®¹
            alt = MIMEMultipart('alternative')
            msg.attach(alt)
            
            # ç”ŸæˆHTMLï¼Œå¦‚æœæœ‰å›¾è¡¨åˆ™åŒ…å«å›¾è¡¨å ä½ç¬¦
            html_body = self._generate_html(papers, query, briefing, derived_queries=derived_queries, has_chart=(chart_img is not None))
            alt.attach(MIMEText(html_body, 'html', 'utf-8'))
            
            # å¦‚æœæœ‰å›¾è¡¨ï¼ŒåµŒå…¥å›¾ç‰‡
            if chart_img:
                img = MIMEImage(chart_img)
                img.add_header('Content-ID', '<trend_chart>')
                msg.attach(img)
            
            # å»ºç«‹è¿æ¥
            print(f"[*] æ­£åœ¨è¿æ¥SMTPæœåŠ¡å™¨...")
            if self.smtp_port in [465, 995]:
                print(f"[*] ä½¿ç”¨SSLè¿æ¥ï¼ˆç«¯å£ {self.smtp_port}ï¼‰")
                server = smtplib.SMTP_SSL(self.smtp_server, self.smtp_port)
            else:
                print(f"[*] ä½¿ç”¨TLSè¿æ¥ï¼ˆç«¯å£ {self.smtp_port}ï¼‰")
                server = smtplib.SMTP(self.smtp_server, self.smtp_port)
                server.starttls()
            
            print("[*] æ­£åœ¨ç™»å½•...")
            server.login(self.sender_email, self.sender_password)
            print("[*] æ­£åœ¨å‘é€é‚®ä»¶...")
            server.send_message(msg)
            server.quit()
            print(f"[*] é‚®ä»¶å·²å‘é€è‡³")
            
        except Exception as e:
            print(f"[!] å‘é€é‚®ä»¶å¤±è´¥: {e}")
            raise


class ArXivAgent:
    """Agent ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, query: str, recipient: str, broad_category: str = "cs", max_results: int = 10, days: int = 3):
        self.query = query
        self.recipient = recipient
        self.broad_category = broad_category
        self.days = days
        
        # 1. åŠ è½½ç”¨æˆ·ç”»åƒ
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        profile_path = os.getenv('USER_PROFILE_PATH')
        if not profile_path:
            # å°è¯•ç›¸å¯¹äºä»£ç æ–‡ä»¶çš„è·¯å¾„ï¼ˆcode/main.py -> ../user_profile.jsonï¼‰
            code_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(code_dir)
            profile_path = os.path.join(parent_dir, 'user_profile.json')
            # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•å½“å‰å·¥ä½œç›®å½•
            if not os.path.exists(profile_path):
                profile_path = 'user_profile.json'
        self.profile_mgr = UserProfileManager(profile_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.fetcher = ArXivPaperFetcher(max_results=max_results)
        # ä¼ é€’ç”¨æˆ·å…´è¶£å’Œç”»åƒç®¡ç†å™¨ç”¨äºæ‰“åˆ†
        user_interest = os.getenv('USER_INTEREST', 'AI for Science')
        self.processor = PaperProcessor(
            profile_manager=self.profile_mgr,
            model=os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
            user_interest=user_interest
        )
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é‚®ä»¶é…ç½®
        smtp_server = os.getenv('SMTP_SERVER', 'smtp.gmail.com')
        smtp_port_str = os.getenv('SMTP_PORT', '587')
        smtp_port = int(smtp_port_str) if smtp_port_str else 587
        sender_email = os.getenv('SENDER_EMAIL')
        sender_password = os.getenv('SENDER_PASSWORD')
        
        if not sender_email or not sender_password:
            raise ValueError("è¯·è®¾ç½®å‘é€è€…é‚®ç®±å’Œå¯†ç ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡SENDER_EMAILå’ŒSENDER_PASSWORDï¼‰")
        
        self.email_sender = EmailSender(
            smtp_server,
            smtp_port,
            sender_email,
            sender_password
        )

    async def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("--- ArXiv Agent v7.0 Started (Personalized Profile Edition) ---")
        
        # 1. ç”»åƒå¢å¼ºï¼šç”Ÿæˆè¡ç”Ÿå…³é”®è¯
        derived_queries = []
        if self.processor.client and self.processor.client.api_key:
            derived_queries = await self.profile_mgr.generate_derived_keywords(
                self.processor.client, 
                self.processor.model
            )
        
        manual_queries = [q.strip() for q in self.query.replace(';', ',').split(',') if q.strip()]
        
        # 2. å¹¶è¡Œä»»åŠ¡ï¼šæ··åˆçˆ¬å–(æ‰‹åŠ¨+AI) + å®è§‚ç»Ÿè®¡
        task_papers = asyncio.to_thread(
            self.fetcher.fetch_papers_mixed, 
            manual_queries, 
            derived_queries, 
            self.days
        )
        task_stats = asyncio.to_thread(
            self.fetcher.fetch_global_stats, 
            self.broad_category, 
            1
        )
        
        papers, stats_global = await asyncio.gather(task_papers, task_stats)
        
        if not papers:
            print("[-] æœªæ‰¾åˆ°è®ºæ–‡ï¼Œç»“æŸä»»åŠ¡ã€‚")
            return

        print(f"[*] æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ï¼ˆæ‰‹åŠ¨: {len([p for p in papers if p.get('source_type') == 'Manual'])}, AIæ¨è: {len([p for p in papers if p.get('source_type') == 'AI Derived'])}ï¼‰ï¼Œå‡†å¤‡è¿›è¡ŒAIåˆ†æ...")

        # 3. æ™ºèƒ½åˆ†æ
        processed_papers = await self.processor.process_batch(papers)
        processed_papers.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        # 4. ç”Ÿæˆå›¾è¡¨å’Œç»¼è¿°
        chart_img = Visualizer.draw_global_trend(stats_global)
        top_3_trends = ", ".join([f"{k}({v})" for k, v in stats_global.most_common(3)])
        briefing = await self.processor.generate_briefing(processed_papers, top_3_trends)
        
        # 5. å‘é€
        high_score_count = sum(1 for p in processed_papers if p.get('score', 0) >= 8)
        subject = f"ArXiv Report: {len(processed_papers)} Papers (Personalized)"
        
        self.email_sender.send_email(
            self.recipient,
            subject,
            processed_papers,
            self.query,
            briefing,
            derived_queries=derived_queries,
            chart_img=chart_img
        )
        print("--- Mission Complete ---")


async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    query = os.getenv('ARXIV_QUERY', 'machine learning, llm agent')
    recipient = os.getenv('RECIPIENT_EMAIL', 'your_email@example.com')
    max_results = int(os.getenv('MAX_RESULTS', '10'))
    days = int(os.getenv('ARXIV_DAYS', '3'))  # é»˜è®¤æŸ¥çœ‹æœ€è¿‘3å¤©
    # å®è§‚åˆ†ç±»ï¼šcs (è®¡ç®—æœº), q-bio (ç”Ÿç‰©), stat (ç»Ÿè®¡), physics (ç‰©ç†)
    broad_category = os.getenv('BROAD_CATEGORY', 'cs')
    
    agent = ArXivAgent(
        query=query,
        recipient=recipient,
        broad_category=broad_category,
        max_results=max_results,
        days=days
    )
    await agent.run()


if __name__ == "__main__":
    # ä½¿ç”¨ asyncio è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())
